{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Day41_train_yolov3_HW.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CG77DrrB2CrU"},"source":["## 作業\n","\n","1. 如何使用已經訓練好的模型？\n","2. 依照 https://github.com/qqwweee/keras-yolo3 的程式碼，請敘述，訓練模型時，資料集的格式是什麼？具體一點的說，要提供什麼格式的文件來描述資料集的圖片以及 bboxes 的信息呢？\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1582609608722,"user_tz":-480,"elapsed":1828,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"id":"NCEP-DG0VxlV","outputId":"66f7fd9b-e03e-481e-946f-eef0dc40b102","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["%tensorflow_version 1.x # 確保 colob 中使用的 tensorflow 是 1.x 版本而不是 tensorflow 2\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n","You set: `1.x # 確保 colob 中使用的 tensorflow 是 1.x 版本而不是 tensorflow 2`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n","1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1582609612490,"user_tz":-480,"elapsed":5543,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"id":"eXT7SQe0KQxv","outputId":"9affdd41-27a0-405a-9d61-c9bca5ad8cb0","colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["!pip install keras==2.2.4 # 需要安裝 keras 2.2.4 的版本"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting keras==2.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n","\r\u001b[K     |█                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 29.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 31.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 34.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 37.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 39.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 38.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 38.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 40.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 40.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 40.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 40.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 40.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 40.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 40.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 40.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 40.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 40.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 40.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 40.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 40.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 40.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 40.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 40.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 40.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 40.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 40.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 40.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 40.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 40.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 40.6MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.2.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1582609634396,"user_tz":-480,"elapsed":27439,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"id":"vELO-PTVxAtm","outputId":"f7ee92d5-de30-48cc-ec72-62ba2abef2e7","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["from google.colab import drive \n","drive.mount('/content/gdrive') # 將 google drive 掛載在 colob，\n","# 下載基於 keras 的 yolov3 程式碼\n","%cd 'gdrive/My Drive'\n","# !git clone https://github.com/qqwweee/keras-yolo3 # 如果之前已經下載過就可以註解掉\n","%cd keras-yolo3"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive\n","/content/gdrive/My Drive/keras-yolo3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"McfLbu4HGKqE","colab_type":"code","colab":{}},"source":["unit_test = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Or9I3CCeG5iA","colab_type":"code","outputId":"0adbae6b-49ca-42cf-9daa-b839c356e8c1","executionInfo":{"status":"ok","timestamp":1582609636425,"user_tz":-480,"elapsed":29333,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'''\n","download yolo weight\n","'''\n","import os\n","\n","# const value\n","# yolo_weight_name = 'yolov3'\n","yolo_weight_name = 'yolov3-tiny'\n","\n","# yolo_cfg_name = 'yolov3.cfg'\n","yolo_cfg_name = 'yolov3-tiny_cus.cfg'\n","\n","def check_yolo_weight():\n","    if not os.path.exists(\"convert.py\") and not os.path.exists(yolo_cfg_name):\n","        print('May not in correct keras-yolo path...')\n","        return False\n","\n","    if not os.path.exists(\"model_data/\" + yolo_weight_name + \".h5\"):\n","        if not os.path.exists(yolo_weight_name + \".weights\"):\n","            print(\"Model doesn't exist, downloading...\")\n","            os.system(\"wget https://pjreddie.com/media/files/\" + yolo_weight_name + \".weights\")\n","        \n","        convert_cmd = \"python convert.py %s %s.weights model_data/%s.h5\" % (yolo_cfg_name, yolo_weight_name, yolo_weight_name)\n","        os.system(convert_cmd)\n","        print(\"Converting %s.weights to %s.h5...\" % (yolo_weight_name, yolo_weight_name))\n","    else:\n","        print('model is existed.')\n","\n","    return True\n","\n","if unit_test:\n","    check_yolo_weight()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["model is existed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KRfpAjBrMM0q","colab_type":"code","outputId":"d8e27c84-e075-4855-f13a-cf1e3f8b8b75","executionInfo":{"status":"ok","timestamp":1582609636426,"user_tz":-480,"elapsed":29301,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'''\n","download dataset\n","'''\n","\n","# const value\n","dataset_cfg = {\n","    \"name\" : 'VOCdevkit',\n","    \"file_name\" : 'VOCtrainval_06-Nov-2007.tar',\n","    \"url_path\" : 'http://pjreddie.com/media/files/'\n","}\n","\n","def check_and_download_dataset(name=None, file_name=None, url_path=None):\n","    if name is None or file_name is None or url_path is None:\n","        print('Bad input of parameter...')\n","        return\n","\n","    if not os.path.exists(name):\n","        os.system(\"wget \" + url_path + file_name) # 下載 VOC 資料集\n","        os.system(\"tar xvf \" + file_name) # 解壓縮資料集，會花幾分鐘\n","    else:\n","        print(\"data exists\")\n","\n","\n","if unit_test:\n","    check_and_download_dataset(**dataset_cfg)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["data exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rcfp5ISwR8By","colab_type":"code","outputId":"64727c9b-1d37-423e-eb5b-6dc9865f6e3d","executionInfo":{"status":"ok","timestamp":1582609636427,"user_tz":-480,"elapsed":29291,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'''\n","generate list file\n","'''\n","import xml.etree.ElementTree as ET\n","from os import getcwd\n","\n","# const value\n","list_train_name = '2007_train_test.txt'\n","list_valid_name = '2007_valid_test.txt'\n","\n","# output_sets = [('2007', 'train', 100), ('2007', 'val', 20)]\n","# classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n","#            \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n","#            \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n","\n","classes = [\"dog\", \"cat\"]\n","\n","img_path_root = 'VOCdevkit/VOC2007/'\n","img_id_sub_path = 'ImageSets/Main/'\n","jpg_sub_path = 'JPEGImages/'\n","annotation_sub_path = 'Annotations/'\n","\n","# img_id_files = ['dog_train.txt', 'dog_val.txt', 'cat_train.txt', 'cat_val.txt']\n","# img_id_files = ['dog_train.txt']\n","\n","img_ids_surffix = [\"_train.txt\", \"_val.txt\"]\n","img_id_files = [ (cls_name + surffix) for cls_name in classes for surffix in img_ids_surffix]\n","\n","def convert_annotation(img_path, list_file_ptr):\n","    input_file = open(img_path)\n","    tree = ET.parse(input_file)\n","    root = tree.getroot()\n","\n","    # search info in element tree\n","    for obj in root.iter('object'):\n","        is_difficult = int(obj.find('difficult').text)\n","        class_name = obj.find('name').text\n","\n","        if is_difficult or class_name not in classes:\n","            continue\n","        cls_id = classes.index(class_name)\n","        xml_bbox = obj.find('bndbox')\n","        xmin = xml_bbox.find('xmin').text\n","        xmax = xml_bbox.find('xmax').text\n","        ymin = xml_bbox.find('ymin').text\n","        ymax = xml_bbox.find('ymax').text\n","        bbox_list = [xmin, ymin, xmax, ymax]\n","        write_str = \" \" + \",\".join([str(int(a)) for a in bbox_list]) + ',' + str(cls_id)\n","        list_file_ptr.write(write_str)\n","\n","def get_img_ids(id_files):\n","    # total train ids\n","    img_ids = list()\n","\n","    for img_id_file in id_files:\n","        img_id_file_str = open(img_path_root + img_id_sub_path + img_id_file).read()\n","        img_id_lines = img_id_file_str.strip().split('\\n')\n","        for id_line in img_id_lines:\n","            id_strs = id_line.split(' ')\n","            if len(id_strs) > 1:\n","                id_str = id_strs[0]\n","                id_bool = int(id_strs[-1])\n","                if id_bool != 1:\n","                    continue\n","            img_ids.append(id_strs[0])\n","\n","    return img_ids\n","\n","def generate_train_valid_file(list_train_name, list_valid_name,\n","                             img_id_files, train_val_ratio=0.8):\n","    # get img ids from file\n","    img_ids = get_img_ids(img_id_files)\n","    img_ids = list(set(img_ids))\n","    train_boundary = round(len(img_ids)*train_val_ratio)\n","    \n","    gen_annotation_file(list_train_name, img_ids[:train_boundary])\n","    gen_annotation_file(list_valid_name, img_ids[train_boundary:])\n","\n","def gen_annotation_file(list_file_name, img_ids):\n","    with open(list_file_name, 'w') as list_file_ptr:\n","        for img_id in img_ids:\n","            # write img path\n","            list_file_ptr.write('./%s%s%s.jpg'%(img_path_root, jpg_sub_path, img_id))\n","            annotation_path = '%s%s%s.xml' % (img_path_root, annotation_sub_path, img_id)\n","            # write annotation info\n","            convert_annotation(annotation_path, list_file_ptr)\n","            list_file_ptr.write('\\n')\n","\n","if unit_test:\n","    if not os.path.exists(list_train_name) or not os.path.exists(list_valid_name):\n","        generate_train_valid_file(list_train_name, list_valid_name, img_id_files)\n","        print('File generate is completed.')\n","    else:\n","        print('File is existed.')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["File is existed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H3qtIO7cAwQZ","colab_type":"code","colab":{}},"source":["# use kmeans.py in keras-yolo\n","from kmeans import YOLO_Kmeans\n","import numpy as np\n","\n","class CusYOLO_Kmeans(YOLO_Kmeans):\n","\n","    def __init__(self, cluster_number, filename, output_name):\n","        super().__init__(cluster_number, filename)\n","        self.filename = filename\n","        self.output_name = output_name\n","    \n","    def result2txt(self, data):\n","        f = open(self.output_name, 'w')\n","        row = np.shape(data)[0]\n","        for i in range(row):\n","            if i == 0:\n","                x_y = \"%d,%d\" % (data[i][0], data[i][1])\n","            else:\n","                x_y = \", %d,%d\" % (data[i][0], data[i][1])\n","            f.write(x_y)\n","        f.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rV15KLZNU9wH","colab_type":"code","outputId":"8c71f3f8-68e0-4e5b-ee43-3d2393bb25a1","executionInfo":{"status":"ok","timestamp":1582609637489,"user_tz":-480,"elapsed":30339,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# generate customer class file and anchor file\n","\n","data_name = 'custom'\n","classes_surffix = '_classes.txt'\n","anchors_surffix = '_anchors.txt'\n","cluster_number = 9\n","anchors_file = 'model_data/' + data_name + anchors_surffix\n","\n","def gen_classes_file(data_name):\n","    with open('model_data/' + data_name + classes_surffix, 'w') as fp:\n","        for cls_name in classes:\n","            fp.write(cls_name + '\\n')\n","\n","def gen_anchor_file(cluster_number, anchors_file):\n","    cus_yolo_inst = CusYOLO_Kmeans(cluster_number, list_train_name, anchors_file)\n","    cus_yolo_inst.txt2clusters()\n","\n","if unit_test:\n","    gen_classes_file(data_name)\n","    gen_anchor_file(cluster_number, anchors_file)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["K anchors:\n"," [[ 70  65]\n"," [111 142]\n"," [168 216]\n"," [211 128]\n"," [242 261]\n"," [321 436]\n"," [326 305]\n"," [392 209]\n"," [450 330]]\n","Accuracy: 77.18%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHZxE9skaHS8","colab_type":"code","outputId":"ac574136-df35-48ff-c3de-8735ca9aa7dd","executionInfo":{"status":"ok","timestamp":1582609639036,"user_tz":-480,"elapsed":31878,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Ready to train\n","from datetime import datetime\n","from train import get_classes, get_anchors, create_model, data_generator_wrapper\n","from keras.optimizers import Adam\n","from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n","\n","\n","# const value\n","train_annotation_path = list_train_name\n","valid_annotation_path = list_valid_name\n","\n","# log_dir = 'logs/' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","log_dir = 'logs/' + datetime.now().strftime(\"%Y-%m-%d\") + '/'\n","classes_path = 'model_data/' + data_name + classes_surffix\n","anchors_path = 'model_data/' + data_name + anchors_surffix"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"87KWzELz7UpC","colab_type":"code","outputId":"6542f1e6-3663-4260-9748-3a8606a5f591","executionInfo":{"status":"ok","timestamp":1582615080108,"user_tz":-480,"elapsed":3237515,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","input_shape = (416,416)\n","batch_size = 8\n","epochs_count = 50\n","epochs_count = 200\n","initial_epoch = 0\n","initial_epoch = 50\n","\n","learning_rate = 1e-3\n","# learning_rate = 1e-4\n","\n","yolo_base_model_path = 'model_data/' + yolo_weight_name +'.h5'\n","yolo_base_model_path = 'logs/2020-02-24/custom_weights_2020-02-24_18-01-07.h5'\n","\n","def get_annotation_lines(annotation_path):\n","    with open(annotation_path) as fp:\n","        ann_lines = fp.readlines()\n","    np.random.shuffle(ann_lines)\n","    \n","    return ann_lines\n","\n","\n","def train_with_data(classes_path, anchors_path, input_shape=(416, 416), is_trainable=True):\n","    class_names = get_classes(classes_path)\n","    num_classes = len(class_names)\n","    anchors = get_anchors(anchors_path)\n","    # load yolo pretrain weight\n","    model = create_model(input_shape, anchors, num_classes, freeze_body=0, weights_path=yolo_base_model_path)\n","    \n","    # log initialize and related setting\n","    if not os.path.exists(log_dir):\n","        os.system(\"mkdir -p \" + log_dir)\n","\n","    logging = TensorBoard(log_dir=log_dir)\n","    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n","    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n","    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1)\n","\n","    train_ann_lines = get_annotation_lines(train_annotation_path)\n","    valid_ann_lines = get_annotation_lines(valid_annotation_path)\n","\n","    num_train = len(train_ann_lines)\n","    num_val = len(valid_ann_lines) \n","\n","    callbacks_list = [logging, checkpoint]\n","    if is_trainable:\n","        callbacks_list.extend([reduce_lr, early_stopping])\n","        # callbacks_list.extend([reduce_lr])\n","        for i in range(len(model.layers)):\n","            model.layers[i].trainable = True\n","\n","    model.compile(optimizer=Adam(lr=learning_rate), loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n","\n","    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n","    model.fit_generator(data_generator_wrapper(train_ann_lines, batch_size, input_shape, anchors, num_classes),\n","            steps_per_epoch=max(1, num_train//batch_size),\n","            validation_data=data_generator_wrapper(valid_ann_lines, batch_size, input_shape, anchors, num_classes),\n","            validation_steps=max(1, num_val//batch_size),\n","            epochs=epochs_count,\n","            initial_epoch=initial_epoch,\n","            callbacks=callbacks_list)\n","    model_file_name = 'custom_weights_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '.h5'\n","    model.save_weights(log_dir + model_file_name)\n","    \n","    return model_file_name\n","\n","if unit_test:\n","    model_file_name = train_with_data(classes_path, anchors_path, input_shape=input_shape)\n","    pass\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","Create YOLOv3 model with 9 anchors and 2 classes.\n","Load weights logs/2020-02-24/custom_weights_2020-02-24_18-01-07.h5.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3080: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","Train on 600 samples, val on 150 samples, with batch size 8.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 51/200\n","75/75 [==============================] - 329s 4s/step - loss: 21.2319 - val_loss: 184.9337\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 52/200\n","75/75 [==============================] - 71s 944ms/step - loss: 20.4776 - val_loss: 17202.1033\n","Epoch 53/200\n","75/75 [==============================] - 71s 945ms/step - loss: 19.4792 - val_loss: 21.7161\n","Epoch 54/200\n","75/75 [==============================] - 71s 943ms/step - loss: 19.0326 - val_loss: 20.0357\n","Epoch 55/200\n","75/75 [==============================] - 71s 948ms/step - loss: 18.4894 - val_loss: 19.1148\n","Epoch 56/200\n","75/75 [==============================] - 71s 948ms/step - loss: 18.1813 - val_loss: 18.7158\n","Epoch 57/200\n","75/75 [==============================] - 71s 950ms/step - loss: 17.9928 - val_loss: 19.1726\n","Epoch 58/200\n","75/75 [==============================] - 71s 949ms/step - loss: 18.0110 - val_loss: 18.1402\n","Epoch 59/200\n","75/75 [==============================] - 71s 951ms/step - loss: 17.9418 - val_loss: 18.2663\n","Epoch 60/200\n","75/75 [==============================] - 72s 954ms/step - loss: 17.6468 - val_loss: 18.4268\n","Epoch 61/200\n","75/75 [==============================] - 71s 952ms/step - loss: 17.2306 - val_loss: 17.2733\n","Epoch 62/200\n","75/75 [==============================] - 71s 951ms/step - loss: 17.4500 - val_loss: 19.2276\n","Epoch 63/200\n","75/75 [==============================] - 71s 952ms/step - loss: 18.9581 - val_loss: 115.8091\n","Epoch 64/200\n","75/75 [==============================] - 71s 951ms/step - loss: 17.7465 - val_loss: 18.0035\n","\n","Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 65/200\n","75/75 [==============================] - 71s 950ms/step - loss: 17.2570 - val_loss: 16.6677\n","Epoch 66/200\n","75/75 [==============================] - 71s 951ms/step - loss: 16.8351 - val_loss: 19.1603\n","Epoch 67/200\n","75/75 [==============================] - 71s 951ms/step - loss: 16.5022 - val_loss: 16.6238\n","Epoch 68/200\n","75/75 [==============================] - 71s 952ms/step - loss: 16.4456 - val_loss: 17.8866\n","Epoch 69/200\n","75/75 [==============================] - 71s 952ms/step - loss: 16.2965 - val_loss: 17.1339\n","Epoch 70/200\n","75/75 [==============================] - 71s 952ms/step - loss: 15.9890 - val_loss: 16.6403\n","\n","Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 71/200\n","75/75 [==============================] - 71s 952ms/step - loss: 15.8230 - val_loss: 16.6178\n","Epoch 72/200\n","75/75 [==============================] - 71s 952ms/step - loss: 15.7564 - val_loss: 15.7835\n","Epoch 73/200\n","75/75 [==============================] - 71s 950ms/step - loss: 15.4578 - val_loss: 15.6920\n","Epoch 74/200\n","75/75 [==============================] - 71s 951ms/step - loss: 15.5600 - val_loss: 14.7900\n","Epoch 75/200\n","75/75 [==============================] - 71s 952ms/step - loss: 15.4302 - val_loss: 15.6764\n","Epoch 76/200\n","75/75 [==============================] - 72s 955ms/step - loss: 15.3837 - val_loss: 15.0762\n","Epoch 77/200\n","75/75 [==============================] - 72s 955ms/step - loss: 15.3574 - val_loss: 15.8128\n","\n","Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 78/200\n","75/75 [==============================] - 72s 954ms/step - loss: 15.4411 - val_loss: 15.1518\n","Epoch 79/200\n","75/75 [==============================] - 72s 954ms/step - loss: 15.0934 - val_loss: 15.2032\n","Epoch 80/200\n","75/75 [==============================] - 72s 954ms/step - loss: 15.0642 - val_loss: 15.8721\n","\n","Epoch 00080: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 81/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.8052 - val_loss: 15.8930\n","Epoch 82/200\n","75/75 [==============================] - 71s 951ms/step - loss: 14.8772 - val_loss: 15.1605\n","Epoch 83/200\n","75/75 [==============================] - 71s 951ms/step - loss: 14.7651 - val_loss: 14.4508\n","Epoch 84/200\n","75/75 [==============================] - 72s 954ms/step - loss: 14.9214 - val_loss: 15.2194\n","Epoch 85/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.6054 - val_loss: 15.7758\n","Epoch 86/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.5135 - val_loss: 13.9072\n","Epoch 87/200\n","75/75 [==============================] - 71s 952ms/step - loss: 14.5172 - val_loss: 14.3367\n","Epoch 88/200\n","75/75 [==============================] - 71s 953ms/step - loss: 14.6012 - val_loss: 14.9101\n","Epoch 89/200\n","75/75 [==============================] - 71s 953ms/step - loss: 14.4337 - val_loss: 14.8254\n","\n","Epoch 00089: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 90/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.3257 - val_loss: 14.3169\n","Epoch 91/200\n","75/75 [==============================] - 72s 956ms/step - loss: 14.3179 - val_loss: 14.2241\n","Epoch 92/200\n","75/75 [==============================] - 71s 953ms/step - loss: 14.2604 - val_loss: 14.0654\n","\n","Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 93/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.2092 - val_loss: 14.2084\n","Epoch 94/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.0924 - val_loss: 14.2365\n","Epoch 95/200\n","75/75 [==============================] - 72s 955ms/step - loss: 13.9439 - val_loss: 14.2751\n","\n","Epoch 00095: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n","Epoch 96/200\n","75/75 [==============================] - 72s 956ms/step - loss: 14.1670 - val_loss: 14.2561\n","Epoch 97/200\n","75/75 [==============================] - 72s 957ms/step - loss: 14.0908 - val_loss: 13.8615\n","Epoch 98/200\n","75/75 [==============================] - 72s 956ms/step - loss: 14.1387 - val_loss: 14.1517\n","Epoch 99/200\n","75/75 [==============================] - 72s 955ms/step - loss: 14.1053 - val_loss: 13.6323\n","Epoch 100/200\n","75/75 [==============================] - 72s 955ms/step - loss: 13.9999 - val_loss: 13.9149\n","Epoch 101/200\n","75/75 [==============================] - 72s 956ms/step - loss: 14.0326 - val_loss: 13.3765\n","Epoch 102/200\n","75/75 [==============================] - 72s 957ms/step - loss: 13.9381 - val_loss: 13.6474\n","Epoch 103/200\n","75/75 [==============================] - 72s 956ms/step - loss: 14.0900 - val_loss: 13.8878\n","Epoch 104/200\n","75/75 [==============================] - 72s 956ms/step - loss: 13.9395 - val_loss: 13.4669\n","\n","Epoch 00104: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n","Epoch 105/200\n","75/75 [==============================] - 72s 956ms/step - loss: 13.8436 - val_loss: 14.6028\n","Epoch 106/200\n","75/75 [==============================] - 72s 958ms/step - loss: 13.9622 - val_loss: 12.5231\n","Epoch 107/200\n","75/75 [==============================] - 72s 958ms/step - loss: 13.9647 - val_loss: 14.2393\n","Epoch 108/200\n","75/75 [==============================] - 72s 958ms/step - loss: 13.8125 - val_loss: 13.6754\n","Epoch 109/200\n","75/75 [==============================] - 72s 958ms/step - loss: 13.8471 - val_loss: 13.5878\n","\n","Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n","Epoch 110/200\n","75/75 [==============================] - 72s 957ms/step - loss: 13.8246 - val_loss: 13.4710\n","Epoch 111/200\n","75/75 [==============================] - 72s 956ms/step - loss: 13.7917 - val_loss: 13.8342\n","Epoch 112/200\n","75/75 [==============================] - 72s 956ms/step - loss: 13.8238 - val_loss: 13.8607\n","\n","Epoch 00112: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n","Epoch 113/200\n","75/75 [==============================] - 72s 957ms/step - loss: 13.7914 - val_loss: 13.6724\n","Epoch 114/200\n","75/75 [==============================] - 72s 956ms/step - loss: 13.8661 - val_loss: 13.5695\n","Epoch 115/200\n","75/75 [==============================] - 72s 958ms/step - loss: 13.8376 - val_loss: 13.9758\n","\n","Epoch 00115: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n","Epoch 116/200\n","75/75 [==============================] - 72s 956ms/step - loss: 13.7325 - val_loss: 13.1503\n","Epoch 117/200\n","75/75 [==============================] - 72s 957ms/step - loss: 13.9105 - val_loss: 13.5914\n","Epoch 118/200\n","75/75 [==============================] - 72s 955ms/step - loss: 13.8810 - val_loss: 14.1709\n","\n","Epoch 00118: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n","Epoch 119/200\n","75/75 [==============================] - 72s 958ms/step - loss: 13.8725 - val_loss: 13.5157\n","Epoch 120/200\n","75/75 [==============================] - 72s 955ms/step - loss: 13.8433 - val_loss: 13.4341\n","Epoch 121/200\n","75/75 [==============================] - 72s 954ms/step - loss: 13.8004 - val_loss: 14.0076\n","\n","Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n","Epoch 00121: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cBmpjefZ2811","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"HnjS9BjZ6vS_","colab_type":"code","colab":{}},"source":["# from yolo import YOLO\n","# from train import get_classes\n","# classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n","#            \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n","#            \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n","\n","# classes_path = 'model_data/voc_classes.txt'\n","# class_names = get_classes(classes_path)\n","# yolo_model = YOLO(model_path='model_data/yolov3.h5', classes_path=classes_path)\n","\n","# cat_img = Image.open('../test_img/001185.jpg')\n","\n","# r_image = yolo_model.detect_image(cat_img)\n","# r_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8efjiQEuVhK","colab_type":"code","outputId":"9c2d4f5a-3b3a-4828-c206-167a88e915c5","executionInfo":{"status":"ok","timestamp":1582617917138,"user_tz":-480,"elapsed":19315,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from yolo import YOLO\n","log_dir = 'logs/2020-02-25/'\n","# yolo_model = YOLO(model_path=log_dir + model_file_name, classes_path=classes_path)\n","yolo_model = YOLO(model_path=log_dir + 'custom_weights_2020-02-25_07-17-58.h5', classes_path=classes_path)\n","# yolo_model = YOLO(model_path=log_dir + 'custom_weights_2020-02-23_05-23-05.h5', classes_path=classes_path)\n","# yolo_model = YOLO(model_path=log_dir + 'custom_weights_2020-02-19_03-06-14.h5', classes_path=classes_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["logs/2020-02-25/custom_weights_2020-02-25_07-17-58.h5 model, anchors, and classes loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xSbmz4sOuoUV","colab_type":"code","outputId":"4c82c793-cdb0-450d-e6b7-d3f1f3a96ce6","executionInfo":{"status":"ok","timestamp":1582617946596,"user_tz":-480,"elapsed":8045,"user":{"displayName":"Hu Kenson","photoUrl":"","userId":"11530365898907296624"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1stN-_3NDrpbr4Awx4VBoHBUZT80Mxoko"}},"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","img_path_root = 'VOCdevkit/VOC2007/'\n","img_id_sub_path = 'ImageSets/Main/'\n","jpg_sub_path = 'JPEGImages/'\n","annotation_sub_path = 'Annotations/'\n","\n","images_path = ['../test_img/001185.jpg', '../test_img/dog2.jpeg', \n","               img_path_root + jpg_sub_path + '007585.jpg',\n","               img_path_root + jpg_sub_path + '007334.jpg', '../test_img/cats.jpg']\n","\n","\n","def show_images(images, is_bgr=False):\n","    plt.figure(figsize=(20,20))\n","    for img_idx, tmp_img in enumerate(images):\n","        print(type(tmp_img))\n","        \n","        tmp_img = np.asarray(tmp_img)\n","        if is_bgr:\n","            tmp_img = tmp_img[:,:,::-1]\n","        print(tmp_img.shape)\n","        plt.subplot(331+img_idx)\n","        plt.imshow(tmp_img)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def detect_images(yolo_model, images_path):\n","    predict_imgs = list()\n","    for image_path in images_path:\n","        tmp_img = Image.open(image_path)\n","        tmp_predict_img = yolo_model.detect_image(tmp_img)\n","        predict_imgs.append(tmp_predict_img)\n","\n","    return predict_imgs\n","\n","if unit_test:\n","    predict_imgs = detect_images(yolo_model, images_path)\n","    show_images(predict_imgs)\n","    pass\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"_wf9dil0vJ-B","colab_type":"code","colab":{}},"source":["# r_image"],"execution_count":0,"outputs":[]}]}